{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, load_dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "\n",
    "#### from disk\n",
    "\n",
    "dataset = load_from_disk(\"D:/proj/datasets/CV17/CV17/TEST/\")\n",
    "\n",
    "def filter_fn(example):\n",
    "    return example['sentence'] is not None\n",
    "\n",
    "dataset = dataset.filter(filter_fn)\n",
    "\n",
    "def preprocess_fn(examples):\n",
    "    audio_tensors = [torch.tensor(audio['array'], dtype=torch.float32) for audio in examples['audio']]\n",
    "    sentence_tensors = [torch.tensor([ord(ch) for ch in sentence], dtype=torch.int64) for sentence in examples['sentence']]\n",
    "    \n",
    "    return {\n",
    "        'audio': audio_tensors,\n",
    "        'sentence': sentence_tensors\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(preprocess_fn, batched=True)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    audio = torch.stack([item['audio'] for item in batch])\n",
    "    sentence = torch.nn.utils.rnn.pad_sequence([item['sentence'] for item in batch], batch_first=True)\n",
    "    return {'audio': audio, 'sentence': sentence}\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### or streaming\n",
    "\n",
    "# Load the dataset with streaming enabled\n",
    "dataset = load_dataset(\"common_voice\", \"en\", split='train', streaming=True)\n",
    "\n",
    "\n",
    "def preprocess_fn(batch):\n",
    "    audio_tensor = torch.tensor(batch['audio']['array'], dtype=torch.float32)\n",
    "    sentence_tensor = torch.tensor([ord(ch) for ch in batch['sentence']], dtype=torch.int64)\n",
    "    return {'audio': audio_tensor, 'sentence': sentence_tensor}\n",
    "\n",
    "\n",
    "\n",
    "class HuggingFaceDataset(IterableDataset):\n",
    "    def __init__(self, hf_dataset, preprocess_fn):\n",
    "        self.dataset = hf_dataset\n",
    "        self.preprocess_fn = preprocess_fn\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in self.dataset:\n",
    "            yield self.preprocess_fn(batch)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    audio = torch.stack([item['audio'] for item in batch])\n",
    "    sentence = torch.nn.utils.rnn.pad_sequence([item['sentence'] for item in batch], batch_first=True)\n",
    "    return {'audio': audio, 'sentence': sentence}\n",
    "\n",
    "streaming_dataset = HuggingFaceDataset(dataset, preprocess_fn)\n",
    "\n",
    "dataloader = DataLoader(streaming_dataset, batch_size=32, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###example trainning loop\n",
    "\n",
    "def train_and_evaluate(model, dataloader, eval_loader, optimizer, loss_fn, scheduler, num_epochs=1, max_steps=None, device='cuda', accumulation_steps=1, clear_cache=True, log_interval=10, eval_interval=20, save_interval=100, checkpoint_dir=\"checkpoint_dir\", log_dir=\"log_dir\"):\n",
    "    model.to(device)\n",
    "    global_step = 0\n",
    "    scaler = torch.amp.GradScaler()\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if max_steps is not None and global_step >= max_steps:\n",
    "            break\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            if max_steps is not None and global_step >= max_steps:\n",
    "                break\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            try:\n",
    "                audio_features = batch['audio'].to(device)\n",
    "                sentences = batch['sentence'].to(device)\n",
    "            except KeyError as e:\n",
    "                print(f\"Key error: {e}. Available keys in batch: {batch.keys()}\")\n",
    "                continue\n",
    "\n",
    "            with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "                with record_function(\"model_training\"):\n",
    "                    with torch.amp.autocast(device_type='cuda'):\n",
    "                        audio_features_encoded = model.encoder(audio_features)\n",
    "                        decoder_output = model.decoder(audio_features_encoded, sentences)\n",
    "                        logits = decoder_output.view(-1, decoder_output.size(-1))\n",
    "                        loss = loss_fn(logits, sentences.view(-1))\n",
    "                        total_loss += loss.item()\n",
    "                        loss = loss / accumulation_steps\n",
    "\n",
    "                    scaler.scale(loss).backward()\n",
    "\n",
    "                    if (step + 1) % accumulation_steps == 0:\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                        if clear_cache:\n",
    "                            torch.cuda.empty_cache()\n",
    "\n",
    "            global_step += 1\n",
    "            end_time = time.time()\n",
    "            samples_per_sec = len(batch['audio']) / (end_time - start_time)\n",
    "\n",
    "            total_norm = 0\n",
    "            for p in model.parameters():\n",
    "                if p.grad is not None:\n",
    "                    param_norm = p.grad.data.norm(2)\n",
    "                    total_norm += param_norm.item() ** 2\n",
    "            total_norm = total_norm ** (1. / 2)\n",
    "\n",
    "            if global_step % log_interval == 0:\n",
    "                writer.add_scalar('Loss/train', total_loss / (step + 1), global_step)\n",
    "                writer.add_scalar('GradientNorm', total_norm, global_step)\n",
    "                writer.add_scalar('LearningRate', optimizer.param_groups[0]['lr'], global_step)\n",
    "                writer.add_scalar('SamplesPerSec', samples_per_sec, global_step)\n",
    "                writer.add_scalar(\"Memory/Allocated\", torch.cuda.memory_allocated(), global_step)\n",
    "                writer.add_scalar(\"Memory/Cached\", torch.cuda.memory_reserved(), global_step)\n",
    "\n",
    "        if global_step % eval_interval == 0:\n",
    "            model.eval()\n",
    "            eval_loss = 0\n",
    "            all_predictions = []\n",
    "            all_labels = []\n",
    "            with torch.no_grad():\n",
    "                for eval_batch in eval_loader:\n",
    "                    try:\n",
    "                        audio_features = eval_batch['audio'].to(device)\n",
    "                        sentences = eval_batch['sentence'].to(device)\n",
    "                    except KeyError as e:\n",
    "                        print(f\"Key error: {e}. Available keys in eval batch: {eval_batch.keys()}\")\n",
    "                        continue\n",
    "\n",
    "                    audio_features_encoded = model.encoder(audio_features)\n",
    "                    decoder_output = model.decoder(audio_features_encoded, sentences)\n",
    "\n",
    "                    logits = decoder_output.view(-1, decoder_output.size(-1))\n",
    "                    loss = loss_fn(logits, sentences.view(-1))\n",
    "                    eval_loss += loss.item()\n",
    "\n",
    "                    all_predictions.extend(torch.argmax(decoder_output, dim=-1).cpu().numpy().tolist())\n",
    "                    all_labels.extend(sentences.cpu().numpy().tolist())\n",
    "\n",
    "            predictions = {\n",
    "                \"predictions\": np.array(all_predictions, dtype=\"object\"),\n",
    "                \"label_ids\": np.array(all_labels, dtype=\"object\")\n",
    "            }\n",
    "\n",
    "            metrics = compute_metrics(predictions)\n",
    "            writer.add_scalar('Loss/eval', eval_loss / len(eval_loader), global_step)\n",
    "            writer.add_scalar('CER', metrics['cer'], global_step)\n",
    "\n",
    "            scheduler.step(eval_loss / len(eval_loader))\n",
    "\n",
    "            sample_indices = range(min(1, len(all_predictions)))\n",
    "            for idx in sample_indices:\n",
    "                pred_str = tokenizer.decode(all_predictions[idx], skip_special_tokens=True)\n",
    "                label_str = tokenizer.decode(all_labels[idx], skip_special_tokens=True)\n",
    "                print(f\"Sample {idx}: Prediction: {pred_str}, Label: {label_str}\")\n",
    "                logging.info(f\"Sample {idx}: Prediction: {pred_str}, Label: {label_str}\")\n",
    "\n",
    "            model.train()\n",
    "\n",
    "        if global_step % save_interval == 0:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_step_{global_step}.pt')\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"Model saved at step {global_step} to {checkpoint_path}\")\n",
    "            logging.info(f\"Model saved at step {global_step} to {checkpoint_path}\")\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}')\n",
    "    logging.info(f'Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}')\n",
    "\n",
    "final_model_path = os.path.join(checkpoint_dir, 'final_model.pt')\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"Final model saved to {final_model_path}\")\n",
    "logging.info(f\"Final model saved to {final_model_path}\")\n",
    "writer.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
